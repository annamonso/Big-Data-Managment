import pandas as pd
import sys
import os
import duckdb
from delta import *
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql import DataFrame

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from utils.batch_ingestion.delta_manager import DeltaLakeManager

class ExploitationZone():
    """
    Contains all functions required for the exploitation zone.
    """

    def __init__(self, duckdb_file_path='/opt/airflow/duckdb_data'):
        """Initialize the Exploitation Zone"""
        self.dataframes = {}
        self.duckdb_path = duckdb_file_path
        self.exploitation_path = "./exploitation_zone"
        self.exploitation_duckdb = duckdb_file_path + '/exploitation/all_data.duckdb'
        self.trusted_duckdb = duckdb_file_path + '/trusted/trusted_zone.duckdb' 

    def get_trusted_zone_data(self):
        """ 
        Obtains all tables in the trusted zone DuckDB and saves them as pandas DataFrames.
        """
        con = duckdb.connect(self.trusted_duckdb)
        try:
            tables = con.execute("SHOW TABLES").fetchall()
            for (table_name,) in tables:
                df = con.execute(f'SELECT * FROM "{table_name}"').fetchdf()
                self.dataframes[table_name] = df
        finally:
            con.close()

    def apply_transformations(self):
        """
        Sets data ready for analytical purposes.
        """
        
        # 1) Save the original data in case analysists want to create a new combination or analyze any data
        self.save_df_to_duckdb()

        # 2) Create a new DuckDB containing data regarding 2025 air quality + happiness 2025
        airquality_2025 = self.dataframes["current_air_quality_data"]
        happiness_2025 = self.dataframes["world_hapiness_scores"]
        join_column = "CountryISO"
        merged_df = pd.merge(airquality_2025, happiness_2025, on=join_column, how="inner")
        output_path = "duckdb_data/exploitation/merged_airquality_happiness_2025.duckdb"
        conn = duckdb.connect(output_path)
        conn.register("merged_df", merged_df)
        conn.execute("CREATE TABLE IF NOT EXISTS merged_airquality_happiness_2025 AS SELECT * FROM merged_df")
        conn.close()

        # 3) HistoricalWeather + AirqualityIndex from 2022 to 2025. (join on year-contry iso code)
        historicalWeather = self.dataframes["historical_weather"]
        airqualityIndex = self.dataframes["air_quality_index"]
        join_column = "CountryISO"
        datetime_column = "DateTime"

        historicalWeather[datetime_column] = pd.to_datetime(historicalWeather[datetime_column])
        airqualityIndex[datetime_column] = pd.to_datetime(airqualityIndex[datetime_column])

        filtered_historicalWeather = historicalWeather[
            (historicalWeather[datetime_column].dt.year >= 2022) &
            (historicalWeather[datetime_column].dt.year <= 2025)
        ]
        filtered_airqualityIndex = airqualityIndex[
            (airqualityIndex[datetime_column].dt.year >= 2022) &
            (airqualityIndex[datetime_column].dt.year <= 2025)
        ]

        merged_hw_aq = pd.merge(
            filtered_historicalWeather,
            filtered_airqualityIndex,
            on=[join_column, datetime_column],
            how="inner"
        )

        output_path_hw_aq = "duckdb_data/exploitation/merged_historicalweather_airquality_2022_2025.duckdb"
        conn2 = duckdb.connect(output_path_hw_aq)
        conn2.register("merged_hw_aq", merged_hw_aq)
        conn2.execute("CREATE TABLE IF NOT EXISTS merged_hw_aq_2022_2025 AS SELECT * FROM merged_hw_aq")
        conn2.close()

        # 4) natural disasters+ hpiness+ temperaturetrends
        temperatureTrends = self.dataframes["historical_weather"]
        airqualityIndex = self.dataframes["air_quality_index"]

    def save_df_to_duckdb(self):
        """
        Saves all pandas dataframes to a DuckDB database.
        """
        if not self.dataframes:
            raise ValueError("No dataframes to save. Please load dataframes first.")

        con = None 
        try:
            print("Starting DuckDB connection for saving...")
            db_dir = os.path.dirname(self.exploitation_duckdb)
            if db_dir:
                os.makedirs(db_dir, exist_ok=True)
            con = duckdb.connect(database=self.exploitation_duckdb, read_only=False)
            print(f"Connected to DuckDB at {self.exploitation_duckdb}")

            # Correct way: iterate over the dict
            for table_name, df in self.dataframes.items():
                if not isinstance(df, pd.DataFrame):
                    raise TypeError(f"Expected a pandas DataFrame for '{table_name}', but got {type(df)}.")

                print(f"Saving DataFrame to DuckDB table: '{table_name}'...")
                con.execute(f'CREATE OR REPLACE TABLE "{table_name}" AS SELECT * FROM df')
                print(f"Successfully saved DataFrame to DuckDB table: '{table_name}'.")

        except Exception as e:
            print(f"Error saving DataFrames to DuckDB: {e}")
            raise ValueError(f"Failed to save DataFrames to DuckDB: {e}") from e 
        finally:
            if con:
                con.close()
                print("Closed DuckDB connection after saving.")

    def print_all_tables_head(self, n=5):
        """
        Prints the first `n` rows of all tables loaded in the exploitation zone.
        """
        if not self.dataframes:
            print("No dataframes loaded. Run get_trusted_zone_data() first.")
            return
        
        for table_name, df in self.dataframes.items():
            print(f"\nTable: {table_name}")
            print(df.head(n))


